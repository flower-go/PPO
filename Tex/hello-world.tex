\documentclass{report}
\usepackage{hyperref}
\begin{document}

\chapter{General MARL/PPO}

\section{The Surprising Effectiveness of MAPPO in Cooperative, Multi-Agent Games}
\url{https://www.researchgate.net/publication/349727671_The_Surprising_Effectiveness_of_MAPPO_in_Cooperative_Multi-Agent_Games}

\begin{list}{}{}
    \item PPO sample efficiency
    \item 1 GPU desktop, 1 Multicore CPU for training
    \item centralized value function - global state s insted of local o
    \item environments: Particle world environment
    \item PPO used be seen as sample less efficient, hence for academic purposes MADDPG anv value-decomposed Q-learning
    \item Minimal hyperparameter tuning and no domain specification
    \item Decentralized learning each agent its own policy, suffer from non-stationary transitions
    \item two lines of research - CTDE (this) and value decomposition
    \item in single agent PG advantage normalization is crucial
    \item considered implementation details - input norm, value clipping, orthogonal init, gradient clip - all helpful and included
    \item another - discretization action space for PPO to avoid bad local minima in continuous, layer normalization
    \item MLP vs Recurrent
    \item 5 implementation details: \newline
        Value norm: running average over value estimates, value network regress to normalized target values (Pop art technique) \newline
        Agent-specific global state: concate of all o\_i as input to critic  \newline
        (agent specific global cannot be used in QMix, which uses single mixer network common to all agents) \newline
        Training Data Usage: importance sampling to perform off-policy correction ?? \newline
            multiple epochs may suffer from non stationarity -> 15 to 5 epochs (easy to hard) \newline
            No mini-batches -> more data to estimate gradients -> imporved practical performance \newline
        Action masking: unavailable actions when computing action probabilities - both forward and backward \newline
        Death masking: zero states with agent ID as value input for dead agents        

\end{list}

\chapter{Overcooked related}
\url{https://github.com/HumanCompatibleAI/overcooked_ai}

\section{On the Utility of Learning about Humans
for Human-AI Coordination}
\url{https://arxiv.org/abs/1910.05789}
\begin{list}{}{}
    \item agents assume their partner to be optimal or similar fail to be understood by humans
    \item gains come from having agent adapt to human's gameplay
    \item effective way to tackle two-player games is train agent with set of other AI agents, often past versions
    \item collaboration is fundamentally different from competition (we need to go beyond self-play to account for human behavior)
    \item incorporating human data or models into training leads to significant improvements (behavior cloning model)
    \item Population Based Training is online evolutionary alg, adapts training hyperparameters and perform model selection
    agents, whose policies are parametrized by NN and trained with DRL alg. each PBT iteration pair of agents are drawn, trained for number of steps and have performance recorded
    at end of PBT iteration, worst performing agents are replaced with copies of best and parameters mutated
    \item human behavior cloning peformed better than with Generative Aversarial Imitation Learning (GAIL)
    \item PBT better than PPO self-play because they are trained to be good with population coordination
    \item Agents designed for humans. Start with one learned human BC as part of environment dynamic and train single agent PPO.
    \item start with ppo self-play and continue with training with human model
    \item planning alg A*
    \item two human behavior cloning models Hproxy used for evaluation and PPOBC learned against learned human models
    \item best result self-play with self-play
    \item for human interaction was best PPOBC with HProxy...PPOBC is overall preferable
    \item PPOBC outperformes human-human performance
    \item SP agents became very specialized and so suffered from distributional shift when paired with human models
    \item future work - better human models, biasing population based training towards humans
    \item READ AGAIN if interested
\end{list}

\section{PantheonRL:
A MARL Library for Dynamic Training Interactions}
\url{https://iliad.stanford.edu/pdfs/publications/sarkar2022pantheonrl.pdf}
\begin{list}{}{}
    \item PantheonRL new software package for marl dynamic
    \item Combination of PettingZoo and RLLIB - customziation of agents
    \item prioritizes modularity of agent objects - each has separate replay buffer, own learning alg, role
    \item (other powerfull DRL library - StableBaselines3)
    \item The modularity of the agent policies com-
    bined with the inheritance of StableBaselines3 capabilities
    together give users a flexible and powerful library for ex-
    perimenting with complex multiagent interactions
    \item 
\end{list}





\section{Investigating Partner Diversification
Methods in Cooperative Multi-agent
Deep Reinforcement Learning}
\url{https://www.rujikorn.com/files/papers/diversity_ICONIP2020.pdf}
\begin{list}{}{}
    \item PBT have diversity problem -> PBT agents are not more robust than self-play agents and aren't better with humans
    \item creating diversity by generating pre-trained partners is simple but effective
    \item (partner sampling - playing with uniformly sampled past versions of partner - lacks diversity, past versions have similar behavior)
    \item (population-base training, pre-trained partners)
    \item testing self-play and cross-play with these agent types (SP, SPpast, PBT, PTseeds, PTdiverse)
    \item PTdiverse(hyperparameters) and PTseeds come from self-play
    \item ref: ustesen, N., Torrado, R.R., Bontrager, P., Khalifa, A., Togelius, J., Risi, S.: Illu-
    minating generalization in deep reinforcement learning through procedural level
    generation. arXiv preprint arXiv:1806.10729 (2018)
    \item IDEA to do: combine pretrained agents with maximum cross entropy? Encorporate maxium cross entropy into ppo??
    \item LOVED THIS ARTICLE for it's simplicity
    
    
\end{list}


\section{Evaluating the Robustness of Collaborative Agents}
\url{https://arxiv.org/abs/2101.05507}
\begin{list}{}{}
    \item how test robustnes if cannot rely on validation reward metric
    \item unit testing (from software engineering) - edge cases, eg. where soup was cooked but not delivered
    \item incorporating Theory of mind to human model 
    \item human modal diversity by using population of human models
    \item state diversity - init from states visited in human-human gameplay
    \item test suite provides significant insight into robustness that is not correlated with average validation reward
    \item "improved robustness as measured by test suite, but decrease in average validation reward"
    \item simple ML metrics are insufficient to capture performance and we must evaluate results base on human judgement
    \item domain randomization - some aspects of env are randomized - behavior can vary significantly based on small randomization of "irrelevant" factors
    \item Theory of mind??? - each step agent decides what task/strategy to do (eg. deliver soup), then choose low-lever action (motion) to persue this goal.
    \item Population of BC models, ToM models, or mixture of two
    \item recurrent networks for all deep rl training procedures
    \item "once the trained policy has
    found a good strategy for getting reward, it is not incentivized to explore other areas of the state space"
    \item sampling initial state from human-human data (diverse starts)
    \item Future work - how evaluate robustness in cases of ambiguous behavior
    \item Or evaluating of proposals that populations with BC had positive effect, but negative for ToM
    \item Meta learning for any kind of game layout, not just those prefabricated (online learning)
\end{list}

\section{Interaction Flexibility in Artificial Agents Teaming with Humans}
\url{https://www.researchgate.net/publication/351533529_Interaction_Flexibility_in_Artificial_Agents_Teaming_with_Humans}
\begin{list}{}{}
    \item too psychological, empirical studies of real people experience when playing with self-play / human BC agents
\end{list}

\section{Maximum Entropy Population-Based Training for Zero-Shot Human-AI Coordination}
\url{https://arxiv.org/abs/2112.11701}
\begin{list}{}{}
    \item "problem of training a Reinforcement Learning (RL) agent that is
    collaborative with humans without using any human data"
    \item "To mitigate this distributional shift, we propose Maximum Entropy Population-
    based training (MEP). In MEP, agents in the population are trained with our
    derived Population Entropy bonus to promote both pairwise diversity between
    agents and individual diversity of agents themselves, and a common best agent is"
    \item Comparing MEP with PPO self-play, PBT, Trajectory diversity and DIctitious CO-play
    \item diverse and distringuishable behaviors between all agent pairs utilizes average KL divergence between all agent poairs
    \item each agent in population is rewarded to maximize centralized population entropy.
    \item we train best response agent by pairing it with with agent sampled by difficulty to collaborate with (prioritizing)
    \item each agent has maximum entropy bonus (to reward) to encourage policy itself to be exploratory
    \item Popuplation diversity = average entropy of each agent + average KL divergence of pairs
    \item Populatino entropy = bounded and efficient surrogate for optimization = entropy of mean policies of population
    \item PE is lower bound for PD
    \item Not using PPO, but custom Entropy loss functions
    \item Population entropy (effective linear pairwise kl divergence) as part of objective
    \item MEP shares intuition with domain randomization... (MEP can be seen as domain randomization technique over partners policies)
    \item (TrajeDi = encourages trajectories diffference between agents - Jensen-SHannon divergence between policies)
    \item (Diversity-Inducing POlicy Gradient = formulated for single agent setting)
    \item Bridges maximum entropy RL and PBT... entropy maximization for achieving robustness
    \item Combining MEP with other MARL algorithms could be Future work
    \item Idea: apply r=r+alpha * population entropy?
    \item Idea: ("Maximum entropy approach adds the dense entropy to the reward for each time step, while entropy regularization adds the mean entropy to the surrogate objective") 
        "Note that
        Entropy regularization is not, in general, equivalent to the maximum entropy objective, which not only optimizes for
        a policy with maximum entropy, but also optimizes the policy itself to visit states where it has high
        entropy. Put another way, the maximum entropy objective optimizes the expectation of the entropy
        with respect to the policyâ€™s state distribution, while entropy regularization only optimizes the policy
        entropy at the states that are visited, without actually trying to modify the policy itself to visit high-
        entropy state"
        \url{https://garage.readthedocs.io/en/latest/user/algo_ppo.html}
    \item 
    \item INTERESTING article
\end{list}

\section{Assisting Unknown Teammates in Unknown Tasks: Ad Hoc Teamwork under Partial Observability}
\url{https://arxiv.org/abs/2201.03538}
\begin{list}{}{}
    \item Ad hoc teamwork under partial observability (ATPO)
    \item unknown teammates performing unknown task withou pre-coordination protocol
    \item ad hoc teamwork has three parts - task identification, teammate identification and planning
    \item 
\end{list}


What is Zero-shot coordination?? - studies how independently trained agents may interact with another on first-attempt
\section{}
\url{}
\begin{list}{}{}
    \item
\end{list}





\end{document}
