\chapter{Our work - Contribution}
In this chapter, we propose a novel approach to injecting explicit diversification into agent behavior through diverse population training.
Previously mentioned traditional population-based methods were not effective enough to add the necessary diversification to the population.
We try to use population training in slight different manner with our primary focus set on two important aspects.
Firsty we try to face well known problems connected with multi agent system by changing our perspective towards single-agent settings and transforming to iterative process.
And secondly we try to address the lack of explicit diversification in standard population-based learning methods by pursuing population policy diversification. 

In following sections we attempt to walk through several different aspects of our population building approach.
These parts can be seen as a soft guideline towards diverse population training algorithm which will be presented at the end of this chapter.

\section{Multi agent setting simplification}

\subsection{Single-agent perspective}
As we could have already seen (\ref{MAS}) multi agent systems can be much more intimidating to tackle down.
One of the proposed suggestion was to try to simplify the sitatio by looking at the multi agent environment from the single-agent point of view where the goal is to learn joint policy.
As discussed before, this is often impossible and mostly inpractical.

Nevertheless we try to, in a sense, attempt to utilize similarly motivated approach.
We propose single-agent learning approach where at any given time only one agent is being learned.
Naturally we cannot simply ommitt the second player from the environment as we cannot deny the multi agent essence from the environment.
We rather look at the environment as being instance of single-agent environment where the parnter is embedded and in a sense forms an integral part of the system.

\subsection{Non-stationarity}
We propose population training to be an iterative process where only one agent is being learned and only after its's training is finished he is included into the population.
Once an agent is added into the population it's policy is fixed and no further updates are applied for the rest of population training process.
Only agents used in environment embedding are the ones coming from the already trained population of agents.
This claim this effectively solves the non-stationarity problem we encountered in multi agent settings as from the point of learning agent the behavior of its partners never changes.

There is unfortunately downside to this approach.
While previously the environment had its state transition function purely deterministic, now with the partner embedded into the system it becomes stochastic.
Nonetheless, we hypothesize that this may also be in favor of the final robustness, since by experiencing stochastic state transitions, the agent could theoretically be more robust as a result, since it will have to learn how to behave in a more stochastic exploratory environment.

\subsection{Population exploitance}
We hypothesize that by utilizing iterative process with past agents being fixed it could also help with population exploitance.
When set of agents are trained concurrently together they may all exploit their behavior towards shared experience.
It could probably happen that the new agents might still try to overfit their behavior towards the population agents.
However, by fixing previous past agents we try to break this relation at least from one side of the couple.
We are optimistic that this, in conjunction with further diversification factors, will greatly reduce exploitation of the population.


\section{Population building}
A mere change in our perception towards single-agent settings would probably not be enough of an improvement towards agent robustness.
Therefore we propose several ideas to promote population diversification.
\subsection{Inicialization}
To start with, inspired by original motivation behind all population-based methods, we follow the idea that populations should be in general always more diverse than single agent.
Hence when utilized in training this diverse robustness should be passed onto the next learned agent.
In order for this to be true the previous condition on non existence of population exploitance must hold.
Additionaly we already proposed iterative training process where we suppose preexistence of some set of agents utilized for partner embeddeding.

We propose that in order for other diversification factors to work we have to start with some initial population for partner embedding sampling that is already somewhat diverse.
One could argue that if we had a method for creating such initial diverse population we could just stop there and extend this population using such method.
However, we do not see the perfect diversity and robustness of initial population to be crucial. 
We rather expect the inital population to introduce somewhat mildly different set of behaviors just to give a resonable starting point for the rest of population training.

\subsection{KL divergence}
We hypothesize that preexistence of diverse population is just a soft precondition.
We propose that the main ingredient behind our diversity building process lies in attempting to shift the behavior of each consequentially trained agent to be as different as possible from the behaviors in the existing population.
Since sampled population partners are embedded into the environment we can look at this approach as domain randomization technique.


To emphasize the difference between two behaviors, we must first be able to measure it.
There are arguably various possible ways how to express difference between two behaviors including methods discussed in evaluation approaches (\ref{RobustnessEvaluation}).
Nevertheless since the agent behavior is represented by parametrized policy we suppose that one of the more straightforward methods for low-level difference could be to look at the difference between the two policy distributions.

When measuring the distance of the probabilty distribution the Kullback-Leibler divergence (\cite{KLDivergence}) often comes in mind.

\[
    D_{KL}(P||Q) = \sum_{x \in X}P(x)\log\left(\frac{P(x)}{Q(x)}\right)
\]

Although it is not a metric since it is not symmetric in the two distributions and also it does not satify the triangle inequality, we can still utilize it as we are mostly interested in relative distribution differences rather than absolute values.
We propose idea of incorporating maximization of KL divergence as a tool to differentiate new agents from the ones that are already contained in the population by taking the average of KL divergence of trained policy and policies from population.

We suggest that there are two different suitable ways how to incorporate KL divergence maximization into the process.
Firstly, KL divergence between policy that is currently being trained and the policies in the population can be used during episode sampling by interpreting it as another additional partial reward based on a current state of the environment.
And secondly, similar approach can be applied on the level of PPO objective function where another additional objective term based on maximization of KL divergence can be added.

It may seem that these two approaches might have the same effect.
However, we believe that they work in a slightly different manner.
When applied in the PPO objective the term is trying to differentiate current policy no matter the current state of the environment.
While applying the KL divergence bonus as partial reward based on the state might work as a exploration enhancing technique since it should push the agent to visit more the states where it's current policy differs the most which could potentially lead to state trajectories that have not been seen by the agents in the population.   

\subsubsection{KL divergence coefficients}
Applying the absolute value of KL divergence the way it is calculated using the formula would not work in our favor as we have to consider the value ranges with regard to the environment properties.
KL divergence is non negative but it can easily reach values of infinity when not bounded.
Additionaly we need to consider suitable multiplicative coefficient to scale the values to reasonable range.

When new parametrized policy $p$ is initialized it produces the probability distribution that is close to uniform.
Having sampled several already trained self-play agents and evaluated in same initial states we obatined specialized policies yielding the probability distributions $q_1, q_2, q_3$ (see table \ref{tab:KLDiv-distributions}).


\begin{table}[htbp]
    \small
    \centering
    \begin{tabular}{lll}
      \toprule
      \                                & Probability distribution     & $D_{KL}(p||q)$         \\ \midrule
      \textit{p}     &$[0.1679, 0.1647, 0.1644, 0.1663, 0.1713, 0.1654]$                                   &                                   \\ \midrule            
      $q_1$                     & $[0.0336, 0.0729, 0.0467, 0.5419, 0.1431, 0.1617]$                            & $0.449$                            \\
      $q_2$                         & $[0.0453, 0.1906, 0.1270, 0.2221, 0.0942, 0.3207]$                            & $0.183$                            \\
      $q_3$                                & $[0.1875, 0.1851, 0.1663, 0.1369, 0.1414, 0.1828]$                         & $0.009$                               \\
      
     \bottomrule
    \end{tabular}
    \caption{KL divergence values ilustrated on trained policies on same random inital state}
    \label{tab:KLDiv-distributions}
\end{table}

These values of KL divergence can give us an idea about the absolute values when new policies are compared to already trained agents.
We believe that probability distributions $p$ a $q_1$ are different enough in terms of what we are trying to achieve when training population of diverse agents.
By considering the absolute value of KL divergence of these two distributions we try to derive suitable KL divergence coefficent for loss and reward augmentation. 

We hypothesize that KL divergence term which we add to PPO objective should have comparable weight as the entropy bonus (recall \ref{PPO}) as this bonus is also designed only to be mild exploratory pressure rather than main objective.
Entropy in early stage of training corresponds to uniform probability distribution which yields absolute values of $1.8$. 
This combined with entropy bonus start coefficient of $0.1$ (hypereparameters table \ref{tab:hyperparameters-algo}) produces absolute values of cca $0.18$.
At the final stage when policy is already trained the entropy usually decreases to absolute values of around $1.2$ which when combined with entropy bonus end coefficent of $0.03$ produces final value of $0.036$.

As far as reward augmentation is considered we believe that when all KL divergence bonus are summed throughout the 400 steps of episode it should correspond only to fraction of what can be obtained by following main objective of soup deliverence.
We propose to design such reward clipping in equivalence to the number of soups delivered ($0.5$, $1$, $1.5$ (R$0$, R$1$, R$2$, respectively)).

After performing several trivial experiments we concluded with following experiment settings (table \ref{tab:KLDiv-coefiicents}) that we want to evaluate during our following experiments.



\begin{table}[htbp]
    \small
    \centering
    \begin{tabular}{lllll}
      \toprule
      Experiment Name        & Reward coefficent     & Reward clip  & Loss coefficent  & Loss clip          \\ \midrule

      No divers.     & $0.0$     & $0.0$   & $0.0$     & $0.0$                        \\\midrule  
      R$0$     & $0.08$     & $0.025$   & $0$     & $0$                        \\
      R$1$     & $0.15$     & $0.05$   & $0$     & $0$                        \\
      R$2$     & $0.1$     & $0.075$   & $0$     & $0$                        \\\midrule
      L$0$     & $0$     & $0$   & $0.08$     & $0.03$                        \\
      L$1$     & $0$     & $0$   & $0.12$     & $0.07$                        \\      
      L$2$     & $0$     & $0$   & $0.1$     & $0.15$                        \\\midrule      
      R$0$L$0$     & $0.08$     & $0.02$   & $0.08$     & $0.02$                        \\
      R$1$L$1$     & $0.1$     & $0.04$   & $0.1$     & $0.03$                        \\
      
     \bottomrule
    \end{tabular}
    \caption{Summary of experiments utilizing KL divergence}
    \label{tab:KLDiv-coefiicents}
\end{table}


\subsection{Population structure}
Finally, we need to provide description of entire proposed population building process.
We have already dealt with population inicialization where we proposed how the inital agents could be constructed and we also designed diversification tools to enhance the behavioral diversity of the population.

However, our ultimate goal is to provide a procedure that produces final agent or set of final agents that should be robust and highly cooperative.
By applying diversifying techniques we effectively train agents that are not fully optimizing towards main objective but their performance can be degraded due to diversification. 

After having trained several diversified population agents we propose to train final agents by exposing them to already built population but ommiting the diversification pressures.
We propose to train first final agent by exposing him with population containing also the agents that were part of inicialization
and then train second agent using same population only with initial agent excluded.

To make sure the final agents have enough space to learn how to cooperate with entire population we extend the number of training timesteps to the factor of $1.5$.


\section{Simple convolution experiments}

\subsubsection{Initial experiments visualised}

After initial population training experiment (figure \ref{CNNPopNoDiffBestInitFixation}) we can see that first three self-play agents used for initialization of population play important role.
In three seeds (10,11,13) initial population apparently strongly represents same one set of behaviors and lacks diversity towards other behaviors.
It appears that as population training progresses, agents learn to fixate solely on this kind of behavior as it promises maximal available outcome and ignore other types of behaviors.

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=14cm]{../img/Forced_coordination_CNN_POP_NO_DIF(2).png}

    \caption{Diversity free populations cross-play evaluation}
    \label{CNNPopNoDiffBestInitFixation}
    \medskip
    \small 
    Cross play evaluation of five populations trained with no diversification pressures against self-play agents

\end{figure}

To confront this issue we propose two suggestions.
Firstly, in these initial experiments first three trained self-play agents were selected for population inicialization automatically.
We instead propose performing initial population agent selection manually by first training five self-play agents, evaluating them using any kind of evaluation method (in our case evaluating them with other set of self-play agents) and finally out of these five selecting such three agents that are both successful and also pair-wise different as much as possible.

And secondly, during training all available agents from current population were present at each training epoch as these were equally distributed among 30 parallel environments.
This guarantes that the dominant type of behavior from population initialization was always present during training, which allowed newly trained agent to move towards this kind of behavior.
Instead we propose not to sample all agents from current population, but rather sample only few to give chance of omitting the dominant ones and allow trained agent to focus also on the remaining types.
After performing simplifed experiment we concluded with suitable value three for the number of sampled partners.  


\begin{figure}[!ht]
    \centering
    \includegraphics*[width=14cm]{../img/SimpleCnnExperimentsAvg.png}

    \caption{Average cummulative reward of final agents}
    \label{AvgCummulativeRewardEvaluated}
    \medskip
    \small 
    Average cummulative reward of the two final agents trained in last phase of the population building.
    Includes values also for cross-play evaluation of self-play evaluation set for reference. 

\end{figure}

\subsubsection{Average outcome evaluation}

As discussed in previous chapters (\ref{RobustnessEvaluation}) performing evaluation of obtained population using usual methods is not very intuitive. 
When overall cross-play average outcome metric is used (figure \ref{AvgCummulativeRewardEvaluated}) there are just mild improvements achieved compared to self-play agents, however results vary a lot which we believe could be attributed to two factors.
Firstly, as we just discussed, the overall cross-play average outcome of agents in the population inicializations also vary a lot among different seeds.
And secondly, even when final population agents are trained there is no guarante, that they will not end up in some suboptimal local optima with respect to current population.


\subsubsection{Ordered average outcome evaluation}
We propose to use a different metric for evaluating final population agents.
After evaluating the cross-play results, we propose to order them in ascending order.
By using this ordering, we lose the information about the variety of specific pairs cooperation in terms of whether different agents managed to cooperate with different set of evaluation agents.
On the other hand, this gives us an idea of how many evaluation agents the evaluated agents are able to cooperate with at a specific cooperation level.

When using this evaluation metric and taking average over results from three different seeds we can see an apparent difference in cooperation curve (see figure \ref{SimpleCNNOrderedAvg}).
Looking at cooperation level with outcome value of 20 we can see that all agents from all of different experiment settings managed to cooperate with much wider set of evaluation self-play agents.
Similar results can be seen also at cooperation level with value of approximately 120.
The most important change in curve progression can be seen at final part of ordered results, where results of self-play agents start to catch up and overcome results of diversified population agents.
We believe that this result is expected as the final part of self-play results corresponds to pair-wise result of self-play agents with similar learned strategy, which should yield strong cooperative outcome.

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/SimpleCNNOrderedAvg.png}

    \caption{Average, ordered cross-play outcomes}
    \label{SimpleCNNOrderedAvg}
    \medskip
    \small 
    Average of ordered results of cross-play evaluation of best population final agents against self-play agents.
    Includes values also for cross-play evaluation of self-play evaluation agent set for reference, where same agent pairs are ommited.
    (Average, 0.25Q, 0.75Q)

\end{figure}

Another interesting results can be seen when applying the same evaluation metric, but rather than looking at the average result, the $0.15$ quantile is used.
By lowering the quantile we are looking at worse case cooperation results obtained on different seeds.
Looking at visualied results (figure \ref{SimpleCNNOrderedAvg0.15Q}) we can see that in worse case there were far less self-play agents that managed to cooperate with other agents at cooperation level value of at least 80.
This result might be considered as an argument towards cooperative abilities of our approach as it shows greater robustness with other agents at lower, yet reasonable cooperation level.



\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/SimpleCNNOrderedAvg0.15Q.png}

    \caption{0.15 Quantile, ordered cross-play outcomes}
    \label{SimpleCNNOrderedAvg0.15Q}
    \medskip
    \small 
    0.15 quantile of ordered results of cross-play evaluation of best population final agents against self-play agents.
    Includes values also for cross-play evaluation of self-play evaluation agent set for reference, where same agent pairs are ommited.
    0.15Q, Min, 0.3Q

\end{figure}

\newpage


\section{Frame stacking}
After performing initial experiments and observing visualized pair-wise results with self-play agents set we often witnessed final agent results which were both unable to cooperate well with dominant behavior from population initialization agents but also unable to learn how to cooperate with rest of behavior types.
As a consequence we started to doubt if our agent architecture potential capabilities are not limited.

So far we tried to expose our new agent to various potentially different kind of behaviors represented by the previous population agents in an attempt to teach him to be capable of cooperation with each of them.
In other words we are trying to learn such policy that best responds best to all previous policies.

However, we claim that this might not be possible when agent have to decide its policy solely based on a current single state of the environment.
To ilustrate an easiest counterexample, we can think any kind of situation where agents get stucked by repeating same actions which results in same state as before.
By looking solely at such particular state we cannot be certain if agents are stucked because we do not know what action is our partner going to do.
However, if we expect our partner to do some action that when combined with our action will lead to some unstucked state, while possibly our agent is expecting us to attempt to do the same, we effectively end up stucked.

To overcome this problem we propose that utilizing some kind of temporal features about previous states is necessary.

TODO: continue here










\subsection{Channels}

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FSChannelsOrderedAvg.png}

    \caption{Average cummulative reward of final agents ordered by evaluation results}
    \label{FSChannelsOrderedAvg}
    \medskip
    \small 
    Average cummulative reward of the best final agent using channels extension for temporal features.    
    (Average, 0.25Q, 0.75Q)

\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FSChannelsOrderedQ15.png}

    \caption{Average cummulative reward of final agents, 0.15quantile}
    \label{FSChannelsOrderedQ15}
    \medskip
    \small 
    Average cummulative reward of the best final agent using tuple of last state frames for temporal features.
    (0.15Q, Min, 0.3Q)

\end{figure}

\newpage

\subsection{Tuple}

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FSTupleOrderedAvg.png}

    \caption{Average cummulative reward of final agents ordered by evaluation results}
    \label{FSTupleOrderedAvg}
    \medskip
    \small 
    Average cummulative reward of the best final agent using channels extension for temporal features.    
    (Average, 0.25Q, 0.75Q)

\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FSTupleOrderedQ15.png}

    \caption{Average cummulative reward of final agents, 0.15quantile}
    \label{FSTupleOrderedQ15}
    \medskip
    \small 
    Average cummulative reward of the best final agent using tuple of last state frames for temporal features.
    (0.15Q, Min, 0.3Q)

\end{figure}

Looks like temporal feature made even basic self-play agent a bit more cooperative (figure \ref{FSVariantsOrderedAvg}).

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FSVariantsOrderedAvg.png}

    \caption{Average cummulative reward of final agents}
    \label{FSVariantsOrderedAvg}
    \medskip
    \small 
    Average cummulative reward of all agents from evaluation set.
    All three types of self-play agent designs included.

    Average, 0.25Q, 0.75Q

\end{figure}


\section{Population evaluation}
In here we will not evaluate agents against self-play agents, but evaluate final population agents against each other among different experiments settings.
(Simple CNN \ref{FinalPopFinalPopSimpleCnnAvg}). (FS Channels \ref{FinalPopFinalPopFsChannelsAvg}). (FS Tuple \ref{FinalPopFinalPopFsTupleAvg})
\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FinalPopFinalPopSimpleCnnAvg.png}

    \caption{Evaluation of final population agent using simple CNN}
    \label{FinalPopFinalPopSimpleCnnAvg}
    \medskip
    \small 

    Average, 0.25Q, 0.75Q

\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FinalPopFinalPopFsChannelsAvg.png}

    \caption{Evaluation of final population agent using channels frame stacking}
    \label{FinalPopFinalPopFsChannelsAvg}
    \medskip
    \small 

    Average, 0.25Q, 0.75Q

\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=10cm]{../img/FinalPopFinalPopFsTupleAvg.png}

    \caption{Evaluation of final population agent using tuple frame stacking}
    \label{FinalPopFinalPopFsTupleAvg}
    \medskip
    \small 

    Average, 0.25Q, 0.75Q

\end{figure}

\newpage

\section{Other layouts}
\subsection{Cramped room}
\begin{figure}[!ht]
    \centering
    \includegraphics*[width=8cm]{../img/CrampedRoomFSVariantsOrderedAvg.png}

    \caption{Average cummulative reward of final agents}
    \label{CrampedRoomFSVariantsOrderedAvg}
    \medskip
    \small 
    Average cummulative reward of all agents from evaluation set.
    All three types of self-play agent designs included.

    Average, 0.25Q, 0.75Q

\end{figure}


\begin{figure}[!ht]
    \centering
    \includegraphics*[width=8cm]{../img/CrampedRoomFSChannelsOrdered15Q.png}

    \caption{Average cummulative reward of final agents ordered by evaluation results}
    \label{CrampedRoomFSChannelsOrdered15Q}
    \medskip
    \small 
    Average cummulative reward of the best final agent using channels extension for temporal features.    
    (Average, 0.25Q, 0.75Q)

\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=8cm]{../img/CrampedRoomFSTupleOrdered15Q.png}

    \caption{Average cummulative reward of final agents, 0.15quantile}
    \label{CrampedRoomFSTupleOrdered15Q}
    \medskip
    \small 
    Average cummulative reward of the best final agent using tuple of last state frames for temporal features.
    (0.15Q, Min, 0.3Q)

\end{figure}

\newpage

\subsection{Counter circuit}

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=8cm]{../img/CounterCircuitFSVariantsOrderedAvg.png}

    \caption{Average cummulative reward of final agents}
    \label{CounterCircuitFSVariantsOrderedAvg}
    \medskip
    \small 
    Average cummulative reward of all agents from evaluation set.
    All three types of self-play agent designs included.

    Average, 0.25Q, 0.75Q

\end{figure}


\begin{figure}[!ht]
    \centering
    \includegraphics*[width=8cm]{../img/CounterCircuitFSChannelsOrderedQ15.png}

    \caption{Average cummulative reward of final agents ordered by evaluation results}
    \label{CounterCircuitFSChannelsOrderedQ15}
    \medskip
    \small 
    Average cummulative reward of the best final agent using channels extension for temporal features.    
    (Average, 0.25Q, 0.75Q)

\end{figure}

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=8cm]{../img/CounterCircuitFSTupleOrderedQ15.png}

    \caption{Average cummulative reward of final agents, 0.15quantile}
    \label{CounterCircuitFSTupleOrderedQ15}
    \medskip
    \small 
    Average cummulative reward of the best final agent using tuple of last state frames for temporal features.
    (0.15Q, Min, 0.3Q)

\end{figure}




% \section{Idea}


% We will transform population learning into an incremental process, where only newly added agents are learned.
% This process is designed to be quite general with respect to the potentially pre-existing set of autonomous agents.

% The building process will start with a population of whatever pre-trained agents are available, no matter what the source of agents is.
% Consequently, a new agent is learned with respect to the previous population by using some diversification techniques.
% After the agent is trained to cooperate with individual agents within the existing population while maintaining its diversity, it is added to the fixed population and the next agent can be trained.

% By fixing the previously trained agent, we effectively solve the problem of non-stationarity, since no two agent policies are changed at the same time.
% By embedding sampled partners from the population into the environment, we can consider the environment as a single agent from the point of view of the agent being trained.
% However, despite the fact that the original overcooked environment is deterministic from the point of view of the transition function, by embedding partners whose policies will be mostly stochastic into the environment, the transition function becomes stochastic from the point of view of the trained agent.
% The same applies to the reward function.
% Since part of our proposed diversification methods involve reward augmentation based on the agent's policy, the environment's reward function will no longer be stationary throughout training due to the learning policy.

% {\color{blue} Po prostudovani a sepsani MARL kapitoly mi pripada, ze muj pristup popsany vyse je vlastne hrozne slaby, obzvlaste tim, ze trenuju ciste reaktivni agenty bez jakekoliv pameti (uvazuji jen soucasnou CNN reprezentaci stavu, bez RNN, bez predeslych akci partnera, bez historie predchozich stavu),
% tim vlastne ani nejsem schopny komplexne zachytit nejakou high-level strategii partnera, namisto toho se proste ucim jak se nejlepe (robustne?) zachovat v jednom konkretnim stavu vzhledem k potencialne vice moznych strategii, kterymi by se mohl partner v danem stavu ridit.
% Tim, ze CNN kompletne popisuje stav prostredi, tak ani nemam partial observability a vstup kritika jsem nijak nerozsiroval.
% Proste pripada mi, ze jsem v tom reseni nezahrnul moc zadne techniky popsane pro MAS.

% Pripada mi ze i tak ty experimenty ktere zkousim a navrhuju jsou zajimave, jen mi pripadaji takove vytrzene z toho MAS kontextu - ale o tom jsme si myslim minule bavili ze nevadi.
% }


% Diverse population can be thought of as domain randomization technique
% "Given just this, it is unclear what the agent should do: the optimal
% policy for the agent depends heavily on the humanâ€™s policy, which the agent has no control over"

% "From the perspective of game theory, we are interested
% in n-person games in which the players have a shared or joint
% utility function. In other words, any outcome of the game has
% equal value for all players. Assuming the game is fully co-
% operative in this sense, many of the interesting problems in
% cooperative game theory (such as coalition formation and ne-
% gotiation) disappear. Rather it becomes more like a standard
% (one-player) decision problem, where the collection of n play-
% ers can be viewed as a single player trying to optimize its be-
% havior against nature." 

% "Solutions to the coordination problem can be divided into
% three general classes, those based on communication, those
% based on convention and those based on learning"

% Convention probably does not make sense as we have ad hoc partner
% Craig Boutilie 1996

% \section{Our definition(s?) of robustness}
% Probably just average of pair results (non diagonal in case of same sets).
% Maybe percentage of pairs who surpassed some threshold reward?

% \section{Population construction}

% \subsection{SP agents initialization}
% \textbf{One agent is not enough?}

% \subsection{population partner sampling during training}
% \textbf{See if playing with whole population at once differs from one random partner for episode}

% \subsection{Final agent training}

% \section{Diverzification}
% \textbf{maximize kl divergence among population partners policies}

% \subsection{Population policies difference rewards augmentation}
 
% \subsection{Population policies difference loss}
