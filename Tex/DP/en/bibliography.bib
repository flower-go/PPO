%
%  An example of a bibliographical database for bibTeX
%
%  Recommended software for maintenance of *.bib files:
%    JabRef, http://jabref.sourceforge.net/
%
%  BEWARE:
%
%    *  If a name contains a capital letter, which must be kept such,
%       use curly brackets ({T}hailand, {HIV}).
%
%  ===========================================================================

@BOOK{Andel07,
  title = {Základy matematické statistiky},
  publisher = {Matfyzpress},
  year = {2007},
  author = {Anděl, J.},
  address = {Praha},
  series = {Druhé opravené vydání},
  isbn = {80-7378-001-1}
}

@BOOK{Andel98,
  title = {Statistické metody},
  publisher = {Matfyzpress},
  year = {1998},
  author = {Anděl, J.},
  address = {Praha},
  series = {Druhé přepracované vydání},
  isbn = {80-85863-27-8}
}

@ARTICLE{Cox72,
  author = {Cox, D. R.},
  title = {Regression models and life-tables (with {D}iscussion)},
  journal = {Journal of the Royal Statistical Society, Series B},
  year = {1972},
  volume = {34},
  pages = {187--220},
  number = {2}
}

@ARTICLE{DempsterLairdRubin77,
  author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
  title = {Maximum likelihood from incomplete data via the {EM} algorithm},
  journal = {Journal of the Royal Statistical Society, Series B},
  year = {1977},
  volume = {39},
  pages = {1--38},
  number = {1}
}

@ARTICLE{Genberget08,
  author = {Genberg, B. L. and Kulich, M. and Kawichai, S. and Modiba, P. and
	Chingono, A. and Kilonzo, G. P. and Richter, L. and Pettifor, A.
	and Sweat, M. and Celentano, D. D.},
  title = {{HIV} risk behaviors in Sub-{S}aharan {A}frica and {N}orthern {T}hailand:
	{B}aseline behavioral data from project {A}ccept},
  journal = {Journal of Acquired Immune Deficiency Syndrome},
  year = {2008},
  volume = {49},
  pages = {309--319}
}

@ARTICLE{KaplanMeier58,
  author = {Kaplan, E. L. and Meier, P.},
  title = {Nonparametric estimation from incomplete observations},
  journal = {Journal of the American Statistical Association},
  year = {1958},
  volume = {53},
  pages = {457--481},
  number = {282}
}

@BOOK{LehmannCasella98,
  title = {Theory of Point Estimation},
  publisher = {Springer-Verlag},
  year = {1998},
  author = {Lehmann, E. L. and Casella, G.},
  address = {New York},
  series = {{S}econd {E}dition},
  isbn = {0-387-98502-6}
}

@ARTICLE{Student08,
  author = {Student},
  title = {On the probable error of the mean},
  journal = {Biometrika},
  year = {1908},
  volume = {6},
  pages = {1-25}
}


@url{DotaOpenFive,
  author          = {OpenAI},
  journal         = {OpenAI Five finals. },
  number          = {},
  title           = {},
  volume          = {},
  year            = {2019},
  url             = {https://openai.com/blog/openai-five-finals/},
}


@url{SpinningUpIntro,
  author          = {Josh Achiam},
  journal         = {},
  number          = {},
  title           = {},
  volume          = {},
  year            = {2018},
  url             = {https://spinningup.openai.com/en/latest/},
}



@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
} 

@article{Atari,
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei and Veness, Joel and Bellemare, Marc and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
year = {2015},
month = {02},
pages = {529-33},
title = {Human-level control through deep reinforcement learning},
volume = {518},
journal = {Nature},
doi = {10.1038/nature14236}
}

@misc{Rainbow,
  doi = {10.48550/ARXIV.1710.02298},
  
  url = {https://arxiv.org/abs/1710.02298},
  
  author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  
  keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{GAE,
  doi = {10.48550/ARXIV.1506.02438},
  
  url = {https://arxiv.org/abs/1506.02438},
  
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  
  keywords = {Machine Learning (cs.LG), Robotics (cs.RO), Systems and Control (eess.SY), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},
  
  title = {High-Dimensional Continuous Control Using Generalized Advantage Estimation},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{SpinningUp2018,
    author = {Achiam, Joshua},
    title = {{Spinning Up in Deep Reinforcement Learning}},
    year = {2018}
}

@misc{TRPO,
  doi = {10.48550/ARXIV.1502.05477},
  
  url = {https://arxiv.org/abs/1502.05477},
  
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Trust Region Policy Optimization},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@book{KLDIV,
  added-at = {2008-09-16T23:39:07.000+0200},
  address = {New York},
  author = {Kullback, Solomon},
  biburl = {https://www.bibsonomy.org/bibtex/28d0af9cdd06af73190b01cc1e04da70b/brian.mingus},
  booktitle = {Information Theory and Statistics},
  description = {CCNLab BibTeX},
  interhash = {03b56ca50da39d05c8832fb6f814ddda},
  intrahash = {8d0af9cdd06af73190b01cc1e04da70b},
  keywords = {stats},
  publisher = {Wiley},
  timestamp = {2008-09-16T23:40:28.000+0200},
  title = {Information Theory and Statistics},
  year = 1959
}


@book{LagrangDuality,
  added-at = {2008-03-02T02:12:02.000+0100},
  address = {Princeton, N. J.},
  author = {Rockafellar, R. Tyrrell},
  biburl = {https://www.bibsonomy.org/bibtex/223aa07ea525f6dd11585fc2037a0daf1/dmartins},
  callnumber = {UniM Maths 516.08 R59},
  description = {robotica-bib},
  interhash = {30830becb0a2c5ebca5946b895d9740a},
  intrahash = {23aa07ea525f6dd11585fc2037a0daf1},
  keywords = {imported},
  notes = {A SRL reference.},
  publisher = {Princeton University Press},
  series = {Princeton Mathematical Series},
  timestamp = {2008-03-02T02:14:11.000+0100},
  title = {Convex analysis},
  year = 1970,
  part= {VI},
}

@article{BacktrackingLineSearch,
author = {Larry Armijo},
title = {{Minimization of functions having Lipschitz continuous first partial derivatives.}},
volume = {16},
journal = {Pacific Journal of Mathematics},
number = {1},
publisher = {Pacific Journal of Mathematics, A Non-profit Corporation},
pages = {1 -- 3},
year = {1966},
doi = {pjm/1102995080},
URL = {https://doi.org/}
}

@misc{PPO,
  doi = {10.48550/ARXIV.1707.06347},
  
  url = {https://arxiv.org/abs/1707.06347},
  
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Proximal Policy Optimization Algorithms},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{CPI,
  title={Approximately Optimal Approximate Reinforcement Learning},
  author={Sham M. Kakade and John Langford},
  booktitle={International Conference on Machine Learning},
  year={2002}
}

@article{EntropyRegularization,
author = {Williams, Ronald J.},
title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
year = {1992},
issue_date = {May 1992},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {8},
number = {3–4},
issn = {0885-6125},
url = {https://doi.org/10.1007/BF00992696},
doi = {10.1007/BF00992696},
abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
journal = {Mach. Learn.},
month = {may},
pages = {229–256},
numpages = {28},
keywords = {gradient descent, mathematical analysis, Reinforcement learning, connectionist networks}
}

@misc{DDPG,
  doi = {10.48550/ARXIV.1509.02971},
  
  url = {https://arxiv.org/abs/1509.02971},
  
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Continuous control with deep reinforcement learning},
  
  publisher = {arXiv},
  
  year = {2015},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{TD3,
  doi = {10.48550/ARXIV.1802.09477},
  
  url = {https://arxiv.org/abs/1802.09477},
  
  author = {Fujimoto, Scott and van Hoof, Herke and Meger, David},
  
  keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Addressing Function Approximation Error in Actor-Critic Methods},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{SAC,
  doi = {10.48550/ARXIV.1812.05905},
  
  url = {https://arxiv.org/abs/1812.05905},
  
  author = {Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Robotics (cs.RO), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Soft Actor-Critic Algorithms and Applications},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{MMDP,
author = {Boutilier, Craig},
title = {Planning, Learning and Coordination in Multiagent Decision Processes},
year = {1996},
isbn = {1558604179},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {There has been a growing interest in AI in the design of multiagent systems, especially in multiagent cooperative planning. In this paper, we investigate the extent to which methods from single-agent planning and learning can be applied in multiagent settings. We survey a number of different techniques from decision-theoretic planning and reinforcement learning and describe a number of interesting issues that arise with regard to coordinating the policies of individual agents. To this end, we describe <i>multiagent Markov decision processes</i> as a general model in which to frame this discussion. These are special <i>n-</i>person cooperative games in which agents share the same utility function. We discuss coordination mechanisms based on imposed conventions (or social laws) as well as learning methods for coordination. Our focus is on the decomposition of sequential decision processes so that coordination can be learned (or imposed) locally, at the level of individual states. We also discuss the use of structured problem representations and their role in the generalization of learned conventions and in approximation.},
booktitle = {Proceedings of the 6th Conference on Theoretical Aspects of Rationality and Knowledge},
pages = {195–210},
numpages = {16},
location = {The Netherlands},
series = {TARK '96}
}

@article{NASH,
author = {John F. Nash },
title = {Equilibrium points in <i>n</i>-person games},
journal = {Proceedings of the National Academy of Sciences},
volume = {36},
number = {1},
pages = {48-49},
year = {1950},
doi = {10.1073/pnas.36.1.48},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.36.1.48},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.36.1.48}}

@article{DecPOMDP,
	doi = {10.1613/jair.2447},
  
	url = {https://doi.org/10.1613%2Fjair.2447},
  
	year = 2008,
	month = {may},
  
	publisher = {{AI} Access Foundation},
  
	volume = {32},
  
	pages = {289--353},
  
	author = {F. A. Oliehoek and M. T. J. Spaan and N. Vlassis},
  
	title = {Optimal and Approximate Q-value Functions for Decentralized {POMDPs}
},
  
	journal = {Journal of Artificial Intelligence Research}
}

@misc{MADDPG,
  doi = {10.48550/ARXIV.1706.02275},
  
  url = {https://arxiv.org/abs/1706.02275},
  
  author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Neural and Evolutionary Computing (cs.NE), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{MAPPO,
  doi = {10.48550/ARXIV.2103.01955},
  
  url = {https://arxiv.org/abs/2103.01955},
  
  author = {Yu, Chao and Velu, Akash and Vinitsky, Eugene and Gao, Jiaxuan and Wang, Yu and Bayen, Alexandre and Wu, Yi},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Multiagent Systems (cs.MA), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{StochasticGame,
author = {L. S. Shapley },
title = {Stochastic Games*},
journal = {Proceedings of the National Academy of Sciences},
volume = {39},
number = {10},
pages = {1095-1100},
year = {1953},
doi = {10.1073/pnas.39.10.1095},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.39.10.1095},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.39.10.1095}}