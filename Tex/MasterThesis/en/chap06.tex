\chapter{Tools and Baselines}\label{BackgroundChapter}
Before we get into our own experiments with training a robust agent, which will be the main focus of the next chapter, we want to prepare our solution first and reimplement some of the previously mentioned results regarding self-play agents.

\section{Framework}
Writing all of our code on top of the Overcooked environment from scratch is not necessary, as there are already several frameworks that implement deep reinforcement learning algorithms.
Although in general there are many more DRL frameworks that could be suitable for our purposes, most related projects using the Overcooked environment have so far used either the RLib library (\cite{liang2018rllib}) or Stable Baselines (\cite{stable-baselines}).

As the authors themselves say, RLlib provides support for production-level and highly distributed RL workloads.
This framework is also suitable for multi-agent learning.
The downside here is that it is less suitable for smaller projects and development on a local machine, as there is a fair amount of overhead due to it's parallelization capabilities towards cluster computing.
Personally, I found the framework a bit intimidating and the documentation a bit confusing.

Another option was the Stable Baselines framework, which in my opinion offers documentation that is clearer and easier to understand.
Also, in my humble opinion, the code base structure is more transparent and basic API is quite light.
To demonstrate its simplicity, once one have an environment that implements the standard RL OpenAI environment interface, one can perform all training with default hyperparameters and policy represented by a multi-layer perceptron network using the PPO algorithm as a simple as follows:
\begin{lstlisting}[language=Python]
    env = make_vec_env("CartPole-v1", n_envs=4)
    model = PPO("MlpPolicy", env, verbose=1)
    model.learn(total_timesteps=25000)

\end{lstlisting}
The downside here is that the framework does not have native support for simultaneous multi-agent learning.

\subsection{Modifications}
After considering both frameworks and the scope of our project, we decided to build our project within the Stable Baselines framework.
We will not cover all the details of our diverse population approach, as there is plenty of room for this in the next chapter.
Here, we just want to mention some of the major framework modifications that had to be carried out in order to bend the framework to suit our purposes.

\subsubsection*{Embedding partner}
First, as mentioned above, the Stable Baselines framework does not support multi-agent learning.
In our approach, we will make a slight simplification by theoretically transforming the environment from a multi-agent to a single-agent setting.
Of course, we will not reduce the number of agents in the environment, as it will still be multi-agent.
Rather, we will look at the situation from the point of view of the single agent that is learning. 
In this way, we can look at the environment as if the partner cook was only embedded as part of the system.

And exactly the same change is necessary in the framework.
During a training run, the partner is embedded as part of the environment, and at each step of the loop, the actions of the partner are sampled.
The actions of the trained agent are sampled in the usual way, as expected, while the actions of the partner cook are sampled according to the policy of the embedded partner.
The resulting two actions are concatenated and passed to the environment as expected.
This mechanism allows us to use any partner sampling method (Section \ref{aiaicooperation}), including self-play, where the same instance of the parameterized policy is used as the one being learned.



\subsubsection*{Convolution policy}
By default, Stable Baselines provides several types of parameterization policy representations, including fully connected dense multi-layer perceptron (MLP) and convolutional neural network (CNN).
However, regarding the CNN representation, when using this wrapper, the framework expects the inputs to be strictly in a standard image format (RGB, RGBD, GrayScale).
Unfortunately, this is accompanied by the use of inadequate assumptions throughout the code base, heuristically expecting the inputs to be in a certain format.
For example, the function to detect if the image space is in the first format only checks if the first dimension is the smallest.
This erroneously returns true for some overcooked layouts using lossless state representation (Subsection \ref{StateRepresentation}) where width is less than both height and the number of stacked masks.
In lossless encoding, masks are equivalent to channels and are represented by the last dimension.
Authors might argue that with such a state representation, the inputs are technically not images, and therefore such sanity checks expecting images should not be used in the first place.
However, this is the problematic part as I was unable to find out whether the framework allows some options to bypass these constraints.
Eventually, after disabling these assertions in certain critical places, the code managed to work flawlessly with our convolutional representation without any further modifications.

\subsubsection*{Loss and rewards augmentation}
Finally, two important aspects of our experiments will revolve around augmenting rewards and extending the objective function.
These modifications are not truly related to Stable Baselines specifically, as these would have to be made regardless of the framework chosen, we just want to mention them here to make the list of changes complete. 
Fortunately, these modifications were also quite straightforward, as both of these elements are nicely separated in transparent, easily extensible locations.

\section{Self-play}
Before we dive into our experiments, we want to make sure that our solution is ready to be used with the environment.
As a reasonable setup, we start by reimplementing basic agent training using self-play (Subsection \ref{selfplayMethod}) and try to demonstrate its allegedly poor cooperation abilities.


\subsection*{Default setting}
We started with the simplest default settings.
For initial experiments we chose to start with the first layout Cramped Room (Subsection \ref{CrampedRoom}) using the more lightweight of the two, partially observable representation (Subsection \ref{StateRepresentation}).
As we recall, this representation uses a one-dimensional vector of features that is out-of-the-box compatible with prefabricated Stable Baselines MLPPolicy wrapper,s which contains two shared hidden layers before separate layers for actor and critic are applied.
Despite several attempts to tweak the hyperparameters, we were not able to come close to the reported results of cooperation failure (Subsection \ref{offDiagonalReport}).
In all of our experiments, different self-playing agents managed to cooperate reasonably well.
While there were some pairs of agents that did not cooperate at all, this was rather rare compared to the overall cross-play evaluation (Figure \ref{MLPSPCrossPlay}).
However, we believe that the problem of uncooperative behavior may be exacerbated by this evaluation scheme in some layouts more than others, since different layouts present different challenges to cooperative behavior.
To make matters worse, while we were able to obtain similar cooperative behavior in the Asymmetric Advantage and Coordination Ring layouts, the same training procedure failed completely in the remaining Forced Coordination and Counter Circuit layouts.
Here, the self-play training procedure was unable to learn how to perform the main task at all.

\begin{figure}[!ht]
    \centering
    \includegraphics*[width=13cm]{../img/MLP_OFF_DIAG_TEST(3).png}
    \caption{Self-play MLP cross play evaluation}
    \label{MLPSPCrossPlay}
    \medskip
    \small 
    Cross-play evaluation of 10 agents trained via self-play method on cramped\_room layout, where policy is parametrized by MLP

\end{figure}

\subsection*{Lossless state representation}
To both overcome this problem and make our experiments comparable to previous related work, we tried to adapt our solution as closely as possible to the settings used in previous work.
We modified our solution based on the original overcooked environment project, whose specific settings are described in detail in their Appendix A (\cite{carroll2020utility}).
Instead of the partially observable state representation, the global lossless representation was adopted.
This implied the need to change the policy parameterization as well.
The Stable Baselines framework provides a prefabricated convolutional CNN policy wrapper.
However, the convolutional part of the network had to be manually modified to match the described structure.
In addition, several PPO hyperparameters have been changed compared to the default stable baseline PPO setting.


\subsection*{Hyperparameters search}
With these settings adjusted, we attempted to run the self-play agent training on all layouts as before.
On the basic Cramped room layout, training seemed to progress successfully, but pairwise cooperation failures were still very rare.
Unfortunately, training with these settings failed on all other layouts.

We performed hyperparameter search by running the training process with several hundred randomly chosen sets of hyperparameters to find a suitable stable configuration.
With the resulting configuration (Table \ref{tab:hyperparameters-algo}) we were able to train a self-play agent on all layouts in most of the runs.
There is only one exception to the table, where we use the value $0.01$ for the VF coefficient on the Cramped room layout.
Without this change, the training was still successful, but the overall outcome of the trained agents was generally lower.

\begin{table}[htbp]
  \small
  \centering
  \begin{tabular}{lll}
    \toprule
    \textbf{Name}                                & Originally proposed     & \textbf{Applied}         \\ \midrule
    \textit{PPO Hyperparameters}     &                                   &                                   \\ \midrule            
    Discount factor $\gamma$                     & $0.99$                            & $0.98$                            \\
    GAE factor $\lambda$                         & $0.98$                            & $0.95$                            \\
    Learning rate                                & $10^{-3}$                         & $4 \cdot 10^{-4}$                               \\
    VF coefficient                               & $0.5$                             & $0.1$                             \\
    PPO clipping                                 & $0.05$                            & $0.1$                             \\
    Maximum gradient norm                        & $0.1$                             & $0.3$                             \\
    Gradient steps per minibatch                 & $8$                               & $8$                               \\      
    Minibatch size                               & $2000$                            & $2000$                            \\
    Number of parallel environments              & $30$                              & $30$                              \\
    Total timestemps                             & $6\cdot10^6$                      & $5.5 \cdot 10^6$                  \\            \\
    Entropy bonus start coefficient              & $0.1$                             & $0.1$                             \\
    Entropy bonus end coefficient                & $0.1$                             & $0.03$                            \\
    Entropy bonus horizon                        & none                              & $1.5\cdot 10^6$                   \\
    Preemptive divergent time step               & $3\cdot 10^6$                     & $3\cdot 10^6$                     \\
    \midrule

    \textit{Environment settings}                &                                   &                                   \\ \midrule                                
    Episode length                               & $400$                             & $400$                             \\
    Soup delivery shared reward                  & $20$                              & $20$                              \\
    Dish pickup partial reward                   & $3$                               & $3$                               \\
    Onion pot placement partial reward           & $3$                               & $3$                               \\
    Soup pickup partial reward                   & $5$                               & $5$                               \\
    Partial reward shaping horizon               & $2.5\cdot 10^6$                   & $2.5\cdot 10^6$                   \\
    Initial state player locations               & static                            & random                            \\
   \bottomrule
  \end{tabular}
  \caption{Hyperparameters for self-play agent training on Cramped room layout}
  \label{tab:hyperparameters-algo}
\end{table}

\subsection*{Preemptive divergent check}
Unfortunately, even this configuration is not completely stable.
In some runs, training on more complex layouts such as Forced Coordination and Counter Circuit fails to achieve the main goal.
In these cases, the agents always learn how to successfully acquire partial rewards by performing the subtasks, but fail to learn the final soup delivery.
We suspect that the combination of layout complexity and likely policy exploitation prevents the agent from experiencing the last part of the task as often as necessary to adapt such behavior.
If this is the case, it is highly unlikely that the agent will discover the soup delivery behavior once the time steps reach the partial reward shaping horizon.
To account for this fact, we perform a preemptive divergence check at the $3\cdot10^6$ time step, where we measure the average number of soups delivered during training.
We have heuristically found that the average soup reward threshold of $3$ over the collected episodes is sufficient to ensure that the agent will learn the main goal in the remaining training process.



  


\subsection{Random state initialization}
Despite these adjustments, there remained problems on two particular layouts.
On the Asymmetric Advantages layout, the resulting performance of the trained agent was significantly worse than in the presented results (\cite{carroll2020utility}).
Whereas on the Forced Coordination layout, training converged to zero performance suspiciously often.
We found that there was a missing constraint on the sampled initial positions of the agents, which allowed the initial positions to be in the same isolated layout region.
While this is not a problem for the remaining layouts, it is an important obstacle for the aforementioned maps.

In both Asymmetric Advantages and Forced Coordination layouts, not all locations are globally reachable.
As a result, in Assymetric Advantages, the agent was forced to learn how to operate within its own region, rather than relying on beneficial cooperation from its partner using its region advantage.
This led to a significantly lower performance value.

In the Forced Coordination layout, the situation was even worse, because if both players started in the same isolated region, they would not be able to finish the soup at all.

Lastly, the authors (\cite{knott2021evaluating}) believe that introducing random initial states can lead to better robustness, which could be an explanation for our better measured cross-play cooperation.
Nevertheless, we ran the same training with fixed initial positions and the results were more or less the same.

\subsection{Focus on Forced coordination}
With all the observations we have made thus far, we can state that there are vast differences between the different layouts. 
Therefore, for our subsequent experiments, we are primarily limiting our attention to one particular layout.
We believe that the Forced Coordination layout provides a good balance between complexity and also emphasizes the importance of cooperation, which is the main point of our interest.
Furthermore, this layout shows signs of a cooperation problem that resembles the reported result (Subsection \ref{offDiagonalReport}) when different self-play agents are evaluated using cross-play (Figure \ref{ForcedCoordinationCNNSPCrossPlay}).

\begin{figure}[!ht]
  \centering
  \includegraphics*[width=14cm]{../img/Forced_coordination_CNN_SP_CrossPlay(5).png}
  \caption{Self-play CNN cross play evaluation on Forced coordination}
  \label{ForcedCoordinationCNNSPCrossPlay}
  \medskip
  \small 
  Cross-play evaluation of 30 agents trained via self-play method on Forced coordination layout, where policy is parametrized by convolutional network

\end{figure}