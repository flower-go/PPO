\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

In recent years, people have witnessed rapid progress in artificial intelligence in all kinds of fields. 
Beating world champions at chess or Go is no longer a problem for AI models. 
The same pattern can be seen in more recent popular games such as Dota (\cite{DotaOpenFive}) or Starcraft, where even great human-AI team cooperation behavior has been achieved.
However, all of these examples share the common trait of being competitive.
The ultimate goal of our society is to create AI that cooperates with humans, not competes with them.

Recent work has shown that cooperative AI models trained together on purely cooperative tasks tend to rely on near-optimal behavior from their partners, and fail to cooperate with partners who don't meet this condition (\cite{carroll2020utility}).
This is bad news for us humans, because our behavior is rarely optimal.

A great example of human-AI cooperation where humans do not always perform perfectly are self-driving cars. 
In a situation where an accident is imminent, humans have to react quickly without having enough time to consider all possible reactions or even analyze the entire current road situation.
However, car accidents are perhaps even too extreme an example of human suboptimal behavior. 
Nevertheless, people often fail even at the simpler task of following standard traffic rules when they have enough time to react.
We can imagine that predicting human behavior is not an easy task for a self-driving car.

In this work, we will first operate in a single-agent environment, revisiting the definition of Markov decision process, the building block of reinforcement learning.
Based on this theory, we will intuitively introduce popular reinforcement learning algorithms divided into two categories of Q-learning and policy optimization.
We primarily focus on policy branch of reinforcement algorithms, especially the algorithm proximal policy optimization, which is considered state of the art algorithm and masively deployed in many successful projects.

Subsequently, we extend the theory developed for single-agent environments to more complex scenarios where more agents are involved.
We highlight problems related to multi-agent settings, where the observability of the world is often an issue compared to single-agent environments.
We illustrate problems with multi-agent training, where changes in one agent affect the behavior of the environment from the point of view of other agents, introducing the problem of non-stationarity.
Finally, some single-agent variants of reinforcement learning algorithms are extended to multi-agent settings, where we mention the important aspects of these extensions.


For our experiments we will use a simplified cooperative cooking environment based on the popular video game Overcooked, where two partners are forced to coordinate a shared task of cooking and delivering soup to a customer.
We will familiarize ourselves with several different kitchen layouts, as each layout may offer different cooperative obstacles.
Here we summarize what approaches have been tested in related work with respect to ad hoc agent cooperation.
We mention the problem of defining the robustness of agent cooperation and different possible definitions of robustness.

And in the last part of this work, we prepare our working environment by modifying the stable-baselines3 library designed for reinforcement learning algorithms.
We try to reimplement some results from previous related work on the problem of ad hoc coordination in AI-AI settings, both for verification purposes and to give us a reasonable baseline for our further experiments.

Finally, for the main goals of this work will attempt to design novel diversifying approach for generating cooperative agents.
We will revisit the problem of robust cooperation evaluation and evaluate our agents for their cooperative abilities.
And finally, we will verify the robustness of our method by evaluating it on other layouts.

\begin{enumerate}
    \item We design a novel diversifying approach for generating cooperative agents.
    \item Evaluation of cooperative abilities
\end{enumerate}

